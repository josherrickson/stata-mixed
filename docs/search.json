[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Modeling in Stata",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#how-to-use-this-document",
    "href": "index.html#how-to-use-this-document",
    "title": "Regression Modeling in Stata",
    "section": "How to use this document",
    "text": "How to use this document\nThese notes are published using Quarto. The Stata code is first rendered using a Stata dynamic document. The source code for these notes can be found at https://github.com/josherrickson/stata-regression for the curious.\nAll images should link to full-size versions to see detail if needed."
  },
  {
    "objectID": "index.html#cscar",
    "href": "index.html#cscar",
    "title": "Regression Modeling in Stata",
    "section": "CSCAR",
    "text": "CSCAR\nhttp://cscar.research.umich.edu/\nCSCAR is available for free consultations with PhD statisticians (email deskpeople@umich.edu to request a consultation).\nCSCAR also has GSRAs available for more immediate help. Walk-ins to our office in Rackham are welcomed Monday-Friday 9am to 5pm (Closed Tuesdays 12-1pm). Alternatively, on our website, you can self-schedule into an hour consultation with the graduate students, which can be either remote or in-person (these are usually available same-day or next-day).\nCSCAR operates a email for help with statistical questions, feel free to send concise questions to stats-consulting@umich.edu.\nThe current contact for questions about the notes: Josh Errickson (jerrick@umich.edu)."
  },
  {
    "objectID": "01-mixed-model-theory.html#terminology",
    "href": "01-mixed-model-theory.html#terminology",
    "title": "1  Mixed Model Theory",
    "section": "1.1 Terminology",
    "text": "1.1 Terminology\nThere are several different names for mixed models which you might encounter, that all fit essentially the same model:\n\nMixed model\nMixed Effects regression/model\nMultilevel regression/model\nHierarchical regression/model (specifically HLM, hierarchical linear model)\n\nThe hierarchical/multilevel variations require thinking about the levels of the data and involves “nesting”, where one variable only occurs within another, e.g. family members nested in a household. The most canonical example of this is students in classrooms, we could have\n\nLevel 1: The lowest level, the students.\nLevel 2: Classroom or teacher (this could also be two separate levels of classrooms inside teacher)\nLevel 3: District\nLevel 4: State\nLevel 5: Country\n\nThis is taking it a bit far; it’s rare to see more than 3 levels, but in theory, any number can exist.\nFor this workshop, we will only briefly discuss this from hierarchical point of view, preferring the mixed models view (with the reminder again that they are the same!).\n\n1.1.1 Econometric terminology\nTo make the terminology a bit more complicated, in econometrics, some of the terms we will use here are overloaded. When you are discussing mixed models with someone with econometric or economics training, it’s important to differentiate between the statistical terms of “fixed effects” and “random effects” which are the two components of a mixed model that we discuss below, and what econometricians called “fixed effects regression” and “random effects regression”.\nWithout going into the full details of the econometric world, what econometricians called “random effects regression” is essentially what statisticians called “mixed models”, what we’re talking about here. The Stata command xtreg handles those econometric models. I have a document which demostrates the equality of these, as well as explaing the ecoonometric “fixed effects regression” in terms of the statistical view of regression."
  },
  {
    "objectID": "01-mixed-model-theory.html#wide-vs-long-data",
    "href": "01-mixed-model-theory.html#wide-vs-long-data",
    "title": "1  Mixed Model Theory",
    "section": "1.2 Wide vs Long data",
    "text": "1.2 Wide vs Long data\nBefore you begin your analysis, you need to ensure that the data is in the proper format. The data can be either in wide-form or long-form. Long-format is sometimes called tall-form.\nIf the data is longitudinal (follows the same person over time, collecting data at intervals), the wide form would entail each row representing a single person, with the variables representing the questions at each wave. For example, if you were asking about income every 2 years, you’d have variables income14, income16, income18 representing the individuals income in 2014, 2016 and 2018.\nThe tall form would have each row of data represent a person and a year. So you’d have a column for ID, a column for year, and then, continuing the example above, a single variable income.\nIf the data is clustered in some sense, e.g. students in classroom, the wide form would have each row be a single classroom, and have a separate set of variables for the first student, second student, etc. In this format, unbalanced data (classrooms having different number of students) would result in a lot of missing values.\nThe tall form would have a single row per student, and a variable representing their class.\nIn a lot of situations, it is easier to collect and manage data in wide form. However, to fit a mixed model, we need the data in long format. We can use the reshape command to transform wide data to long. This is covered in my Introduction to Stata set of notes."
  },
  {
    "objectID": "01-mixed-model-theory.html#level-1-versus-level-2-variables",
    "href": "01-mixed-model-theory.html#level-1-versus-level-2-variables",
    "title": "1  Mixed Model Theory",
    "section": "1.3 Level-1 versus Level-2 variables",
    "text": "1.3 Level-1 versus Level-2 variables\nOften you will have variables that measure something about the unit of analysis, and other variables which measure something about the grouping variable. For example, you may have a variable indicating the GPA of a student, another variable indicating the size of their classroom, and yet another variable indicating the percent minority in their school.\nWhen thinking about this from the hierarchical view, it’s important to differentiate between these types of variables. If you take my advice and think of this from the mixed model point of view, the difference is irrelevant - each is just a variable associated with the student.\nThe only thing that matters is ensuring the data is correct - if you had 10 students in a class, and the variable sizeofclass was 10 for half them and 8 for the other half, Stata wouldn’t know this is an issue and would fit the model without complaining - but now you’ve got a data issue that could be affecting your analysis.\nWhen the data is repeated measures over time, these are sometimes called time-variant (e.g. patient follow-ups, measured at each follow-up) or time-invariant (baseline characteristics or immutable demographics)."
  },
  {
    "objectID": "01-mixed-model-theory.html#theory",
    "href": "01-mixed-model-theory.html#theory",
    "title": "1  Mixed Model Theory",
    "section": "1.4 Theory",
    "text": "1.4 Theory\nThe equation for ordinal least squares (linear regression) is\n\\[\n  Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi} + \\epsilon_i\n\\]\nwhere \\(Y_i\\) represents the response of individual \\(i\\), the various \\(X_{ki}\\) represent the predictor variables for the same respondent, the \\(\\beta_j\\) are the coefficients to be estimated (which are constant across individuals). \\(\\epsilon_i\\) is the additional error for this individual.\nAs mentioned above, there are two ways to think about mixed models - as a mixed model, or as a hierarchical model. Let’s talk about the mixed model first.\nThe most basic form of a mixed model (which is also the most commonly used form) modifies the regression model above by separating the error term \\(\\epsilon_i\\) into a contribution to the error from each level of the data:\n\\[\n  Y_{ij} = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi} + \\kappa_j + \\epsilon_{ij}\n\\]\nThe subscript notation helps us keep track of things. Here, I’ve set it up such that each observation \\(i\\) belongs to a group \\(j\\). The response belonging to individual \\(i\\) in group \\(j\\) is predicted based upon some \\(X\\) variables, plus some additive effect which is unique to group \\(j\\) (\\(\\kappa_j\\)), and additional error unique to that individual ($_{ij}).1\nBoth error terms, \\(\\epsilon\\) and \\(\\kappa\\), are assumed to have a mean of 0. This is important here because it means that on average the random intercept has no affect, but varies from individual to individual.\nThe \\(\\beta\\) terms are the fixed effects, and the \\(\\kappa\\) error terms are the random effects.\nThe catch is that we do not estimate \\(\\kappa_j\\). If we include a categorical variable for the grouping variable as a fixed effect, we could estimate all those intercepts, however, doing so would in most situations overfit the model (if each individual had two measurements, we’d be including \\(n/2\\) predictors in a model with \\(n\\) observations). Instead, we estimate only the variance of \\(\\kappa_j\\) which allows us to determine whether the intercepts differ between groups.\nLet’s use a concrete example to make this more precise. Let your data set consist of \\(n\\) students, labeled \\(s = 1, 2, \\cdots, n\\), each belonging to one of \\(m\\) classrooms, labeled \\(c = 1, 2, \\cdots, m\\). For further simplicity, let’s assume there is only a single fixed predictor \\(X\\).\n\\[\n  Y_{sc} = \\beta_0 + \\beta_1X_s + \\kappa_c + \\epsilon_{sc}\n\\]\nYou predict \\(Y\\) based upon the \\(X\\)’s, then there is some common error amongst all students in classroom \\(c\\) which captured by \\(\\kappa_c\\), then there is individual error captured by \\(\\epsilon_{sc}\\).\n(You sometimes see the \\(\\kappa_j\\) term written as \\(\\kappa_j Z_j\\) where \\(Z_j\\) would be the variable indicating group membership. I find the above notation clearer, though they are mathematically equivalent.)\nIn the above example, I’ve assumed that the sole \\(X\\) in the model is measured as the student level (hence \\(X_s\\)). There is no need for that. I could instead fit the above model with one variable measured per student (say GPA) and one variable measured per classroom (say average teacher evaluation).\n\\[\n  Y_{sc} = \\beta_0 + \\beta_1X_{1s} + \\beta_2X_{2c} + \\kappa_c + \\epsilon_{sc}\n\\]\nThe nice part is that when fitting the model, this distinction doesn’t matter! Both \\(X_{1s}\\) and \\(X_{2c}\\) are treated the same way.\nThe error terms can be expanded if desired. For students (\\(s\\)) nested inside classrooms (\\(c\\)) nested inside districts (\\(d\\)):\n\\[\n  Y_{scd} = \\beta_0 + \\beta_1X_{1s} + \\beta_2X_{2c} + \\gamma_d + \\kappa_{cd} + \\epsilon_{scd}\n\\]\nHere \\(\\gamma_d\\) is the error common to all students in a given district, \\(\\kappa_{cd}\\) is the additional error common to all students in a given classroom, and \\(\\epsilon_{scd}\\) is any left over student error.\n\n1.4.1 The Hierarchical framework\nThe hierarchical way to write these models is, in my opinion, unnecessarily complicated and does not improve on the understanding. It is more complicated for two reasons: 1) it is simply more complicated to write and understand, and 2) it requires conceptualizing the levels of the model more than needed. For completeness, I will briefly re-write the school and classroom above. In mixed form, the model would be:\n\\[\n  Y_{sc} = \\beta_0 + \\beta_1X_{1s} + \\kappa_c + \\epsilon_{sc}\n\\]\nIn the hierarchical form, the model is:\n\\[\n  Y_{sc} = \\beta_{s0} + \\beta_1X_{1s} + \\epsilon_{sc}\n\\] \\[\n  \\beta_{s0} = \\gamma_{0} + \\gamma_{1}Z_c + \\sigma_c\n\\]\nIf you were to plug \\(\\beta_{s0}\\) back into the first equation, you can see the equivalence of the two forms."
  },
  {
    "objectID": "01-mixed-model-theory.html#footnotes",
    "href": "01-mixed-model-theory.html#footnotes",
    "title": "1  Mixed Model Theory",
    "section": "",
    "text": "The choice of \\(\\epsilon\\) for the individual error in a regression is fairly standardized in the literature. My choice of \\(\\kappa\\) is not, as the literature does not have any standard choice for the random effect.↩︎"
  },
  {
    "objectID": "02-fitting-mixed-models.html#fitting-the-model",
    "href": "02-fitting-mixed-models.html#fitting-the-model",
    "title": "2  Fitting Linear Mixed Models",
    "section": "2.1 Fitting the model",
    "text": "2.1 Fitting the model\nFirst, let’s fit a linear regression model ignoring the dependence within households.\n. regress qol age agebelow52 ageabove82 i.socialclass female\n\n      Source |       SS           df       MS      Number of obs   =     5,179\n-------------+----------------------------------   F(9, 5169)      =     16.52\n       Model |  9173.37321         9  1019.26369   Prob &gt; F        =    0.0000\n    Residual |  318919.729     5,169  61.6985354   R-squared       =    0.0280\n-------------+----------------------------------   Adj R-squared   =    0.0263\n       Total |  328093.103     5,178  63.3629012   Root MSE        =    7.8548\n\n------------------------------------------------------------------------------\n         qol | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   -.010319   .0135138    -0.76   0.445    -.0368117    .0161737\n  agebelow52 |  -.5032791    .559864    -0.90   0.369    -1.600849    .5942913\n  ageabove82 |  -1.479405   1.006563    -1.47   0.142    -3.452694    .4938839\n             |\n socialclass |\nManageria..  |   .2622181   .5266783     0.50   0.619    -.7702942     1.29473\n Non-Manual  |  -1.227245    .525863    -2.33   0.020    -2.258159   -.1963312\n    Skilled  |    -2.1252   .5508147    -3.86   0.000     -3.20503    -1.04537\nSemi-skil~d  |  -3.091173   .5596054    -5.52   0.000    -4.188236   -1.994109\n  Unskilled  |  -4.056536   .7457017    -5.44   0.000    -5.518427   -2.594645\n             |\n      female |   .4453072   .2310058     1.93   0.054     -.007562    .8981763\n       _cons |   45.07279   1.002216    44.97   0.000     43.10802    47.03756\n------------------------------------------------------------------------------\nThis model doesn’t do so hot, but it’s sufficient for our purposes - the F-test rejects.\nThe interpretation of the age variables is that the coefficient on age represents the relationship between age and QoL for individuals between ages 52 and 81. The two coefficients on agebelow52 and ageabove82 are allowing those individuals with censored ages to have a unique intercept, which means they don’t affect the slope on age. If you really wanted to drill down into what this all means, you could do some fancy margins calls to predict the average response using at() to force the two dummies to the appropriate levels (not run):\nmargins, at(age = 51 agebelow52 = 1 ageabove82 = 0) ///\n         at(age = (52 81) agebelow52 = 0 ageabove82 = 0) ///\n         at(age=81 agebelow52 = 0 ageabove82 = 1)\nmarginsplot\nIn this case, there doesn’t seem to be much effect of age (though it is good we controlled for it!).\nWe see a marginal effect for female, and we see some differences amongst social classes. Let’s explore them more with margins:\n. margins socialclass\n\nPredictive margins                                       Number of obs = 5,179\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n socialclass |\nProfessio~l  |   44.61599   .4839074    92.20   0.000     43.66733    45.56465\nManageria..  |   44.87821   .2053926   218.50   0.000     44.47555    45.28086\n Non-Manual  |   43.38874   .1976598   219.51   0.000     43.00125    43.77624\n    Skilled  |   42.49079   .2765152   153.67   0.000      41.9487    43.03288\nSemi-skil~d  |   41.52482   .2788682   148.90   0.000     40.97812    42.07152\n  Unskilled  |   40.55945   .5653632    71.74   0.000      39.4511     41.6678\n------------------------------------------------------------------------------\n\n. margins socialclass, pwcompare(pv)\n\nPairwise comparisons of predictive margins               Number of obs = 5,179\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\n\n------------------------------------------------------------------------------\n                                      |            Delta-method    Unadjusted\n                                      |   Contrast   std. err.      t    P&gt;|t|\n--------------------------------------+---------------------------------------\n                          socialclass |\n              Managerial & Technical  |\n                                  vs  |\n                        Professional  |   .2622181   .5266783     0.50   0.619\n          Non-Manual vs Professional  |  -1.227245    .525863    -2.33   0.020\n             Skilled vs Professional  |    -2.1252   .5508147    -3.86   0.000\n        Semi-skilled vs Professional  |  -3.091173   .5596054    -5.52   0.000\n           Unskilled vs Professional  |  -4.056536   .7457017    -5.44   0.000\nNon-Manual vs Managerial & Technical  |  -1.489463   .2842616    -5.24   0.000\n   Skilled vs Managerial & Technical  |  -2.387418   .3458945    -6.90   0.000\n                        Semi-skilled  |\n                                  vs  |\n              Managerial & Technical  |  -3.353391   .3461459    -9.69   0.000\n Unskilled vs Managerial & Technical  |  -4.318754   .6011768    -7.18   0.000\n               Skilled vs Non-Manual  |  -.8979548   .3446961    -2.61   0.009\n          Semi-skilled vs Non-Manual  |  -1.863928   .3411728    -5.46   0.000\n             Unskilled vs Non-Manual  |  -2.829291   .5978386    -4.73   0.000\n             Semi-skilled vs Skilled  |  -.9659729   .3939009    -2.45   0.014\n                Unskilled vs Skilled  |  -1.931336   .6317203    -3.06   0.002\n           Unskilled vs Semi-skilled  |   -.965363   .6305738    -1.53   0.126\n------------------------------------------------------------------------------\nSo Professional and Managerial are indistinguishable, and Semi-skilled and Unskilled are likewise indistinguishable.\nTo fit the mixed model, the command is mixed. Let’s first fit it again ignoring the household random effects.\n. mixed qol age agebelow52 ageabove82 i.socialclass female\n\nMixed-effects ML regression                             Number of obs =  5,179\n                                                        Wald chi2(9)  = 148.97\nLog likelihood = -18018.271                             Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n         qol | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   -.010319   .0135007    -0.76   0.445    -.0367799     .016142\n  agebelow52 |  -.5032791   .5593233    -0.90   0.368    -1.599533    .5929744\n  ageabove82 |  -1.479405    1.00559    -1.47   0.141    -3.450326    .4915162\n             |\n socialclass |\nManageria..  |   .2622181   .5261696     0.50   0.618    -.7690553    1.293492\n Non-Manual  |  -1.227245    .525355    -2.34   0.019    -2.256922   -.1975681\n    Skilled  |    -2.1252   .5502827    -3.86   0.000    -3.203734   -1.046666\nSemi-skil~d  |  -3.091173   .5590649    -5.53   0.000     -4.18692   -1.995426\n  Unskilled  |  -4.056536   .7449814    -5.45   0.000    -5.516673   -2.596399\n             |\n      female |   .4453072   .2307827     1.93   0.054    -.0070187     .897633\n       _cons |   45.07279   1.001248    45.02   0.000     43.11038     47.0352\n------------------------------------------------------------------------------\n\n------------------------------------------------------------------------------\n  Random-effects parameters  |   Estimate   Std. err.     [95% conf. interval]\n-----------------------------+------------------------------------------------\n               var(Residual) |    61.5794   1.210117      59.25271    63.99746\n------------------------------------------------------------------------------\nWe get identical results. If you look at the equations again, when there are no random effects, the model simplifies to ordinal least squares.\nTo add our random effect, we’ll use the following notation:\nmixed y &lt;fixed effects&gt; || &lt;group variable&gt;:\nThe || splits a formula into two sides, the left of it is the fixed effects, the right is the random effects. The : is for including random slopes which we’ll discuss below.\n. mixed qol age agebelow52 ageabove82 i.socialclass female || household:\n\nPerforming EM optimization ...\n\nPerforming gradient-based optimization: \nIteration 0:  Log likelihood = -17951.595  \nIteration 1:  Log likelihood = -17945.184  \nIteration 2:  Log likelihood = -17945.183  \n\nComputing standard errors ...\n\nMixed-effects ML regression                          Number of obs    =  5,179\nGroup variable: household                            Number of groups =  3,995\n                                                     Obs per group:\n                                                                  min =      1\n                                                                  avg =    1.3\n                                                                  max =      3\n                                                     Wald chi2(9)     = 129.84\nLog likelihood = -17945.183                          Prob &gt; chi2      = 0.0000\n\n------------------------------------------------------------------------------\n         qol | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |  -.0102437   .0139236    -0.74   0.462    -.0375335    .0170461\n  agebelow52 |   -.379831   .5276012    -0.72   0.472     -1.41391    .6542483\n  ageabove82 |  -1.210758    .986185    -1.23   0.220    -3.143646    .7221287\n             |\n socialclass |\nManageria..  |   .2347399   .5116288     0.46   0.646    -.7680341    1.237514\n Non-Manual  |   -1.09934   .5130227    -2.14   0.032    -2.104846   -.0938337\n    Skilled  |  -1.802139   .5375521    -3.35   0.001    -2.855722   -.7485564\nSemi-skil~d  |   -2.97498   .5493453    -5.42   0.000    -4.051677   -1.898283\n  Unskilled  |  -3.555772   .7322568    -4.86   0.000    -4.990969   -2.120575\n             |\n      female |   .5708249   .2086576     2.74   0.006     .1618636    .9797862\n       _cons |   44.78498   1.022019    43.82   0.000     42.78186     46.7881\n------------------------------------------------------------------------------\n\n------------------------------------------------------------------------------\n  Random-effects parameters  |   Estimate   Std. err.     [95% conf. interval]\n-----------------------------+------------------------------------------------\nhousehold: Identity          |\n                  var(_cons) |   22.93389    1.79542      19.67161    26.73718\n-----------------------------+------------------------------------------------\n               var(Residual) |   38.98733   1.613297      35.95015     42.2811\n------------------------------------------------------------------------------\nLR test vs. linear model: chibar2(01) = 146.18        Prob &gt;= chibar2 = 0.0000\nLet’s walk through the output. Note that what we are calling the random effects (e.g. individuals in a repeated measures situation, classrooms in a students nested in classroom situation), Stata refers to as “groups” in the top of the output.\n\nYou probably noticed how slow it is - at the very top, you’ll see that the solution is arrived at iteratively. Recall that any regression model aside from OLS requires an iterative solution.\nThe log likelihood is how the iteration works; essentially the model “guesses” choices for the coefficients, and finds the set of coefficients that minimize the log likelihood. Of course, the “guess” is much smarter than random. The actual value of the log likelihood is meaningless.\nSince we are dealing with repeated measures of some sort, instead of a single sample size, we record the total number of obs, the number of groups (unique entries in the random effects) and min/mean/max of the groups. Just ensure there are no surprises in these numbers. In this model, the quality of life has a good chunk of missingness, so we’re losing about 2000 individuals.\nThe \\(\\chi^2\\) test tests the hypothesis that all coefficients are simultaneously 0.\n\nWe have a significant p-value, so we continue with the interpretation.\n\nThe coefficients table is interpreted just as in linear regression, with the addendum that each coefficient is also controlling for the structure introduced by the random effects.\n\nThere is still nothing informative in age, however, compared to OLS, the two dummy variables have notably different coefficients.\nWe’ll have to check whether there is any difference in our conclusion regarding social class.\nThe coefficient on gender is larger and much more significant.\n\nThe second table (“Random-effects parameters”) gives us information about the error structure. The household: section is examining whether there is variation across households above and beyond the differences in the controlled variables. Since the estimate of var(_cons) (the estimated variance of the constant per person - the individual level random effect) is non-zero (and not close to zero), that is evidence that the random effect is necessary. If the estimate was 0 or close to 0, that would be evidence that the random effect is unnecessary and that any difference between individuals is already accounted for by the covariates.\nThe estimated variance of the residuals is any additional variation between observations. This is akin to the residuals from linear regression.\nThe \\(\\chi^2\\) test at the bottom is a formal test of the inclusion of the random effects versus a linear regression model without the random effects. We reject the null that the models are equivalent, so it is appropriate to include the random effects.\n\nLet’s quickly examine the social class categories. The calls to margins are identical, but they operate slightly differently with the random effects. Recall that the margins command works by assuming every row data is a given social class, then uses the observed values of the other fixed effects and the regression equation to predict the outcome, and averaging to obtain the marginal means. This is not the case with the random effects; the random effects (\\(\\kappa_j\\)) are assumed to be 0. So for any given household, the predicted response could be higher or lower, but on aggregate across all households, we are estimating the marginal means.\n. margins socialclass\n\nPredictive margins                                       Number of obs = 5,179\n\nExpression: Linear prediction, fixed portion, predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n socialclass |\nProfessio~l  |   44.41056   .4747326    93.55   0.000      43.4801    45.34101\nManageria..  |    44.6453    .207459   215.20   0.000     44.23868    45.05191\n Non-Manual  |   43.31122    .198432   218.27   0.000      42.9223    43.70014\n    Skilled  |   42.60842   .2721936   156.54   0.000     42.07493    43.14191\nSemi-skil~d  |   41.43558   .2762047   150.02   0.000     40.89422    41.97693\n  Unskilled  |   40.85478    .555399    73.56   0.000     39.76622    41.94335\n------------------------------------------------------------------------------\n\n. margins socialclass, pwcompare(pv)\n\nPairwise comparisons of predictive margins               Number of obs = 5,179\n\nExpression: Linear prediction, fixed portion, predict()\n\n------------------------------------------------------------------------------\n                                      |            Delta-method    Unadjusted\n                                      |   Contrast   std. err.      z    P&gt;|z|\n--------------------------------------+---------------------------------------\n                          socialclass |\n              Managerial & Technical  |\n                                  vs  |\n                        Professional  |   .2347399   .5116288     0.46   0.646\n          Non-Manual vs Professional  |   -1.09934   .5130227    -2.14   0.032\n             Skilled vs Professional  |  -1.802139   .5375521    -3.35   0.001\n        Semi-skilled vs Professional  |   -2.97498   .5493453    -5.42   0.000\n           Unskilled vs Professional  |  -3.555772   .7322568    -4.86   0.000\nNon-Manual vs Managerial & Technical  |   -1.33408   .2766414    -4.82   0.000\n   Skilled vs Managerial & Technical  |  -2.036879   .3383401    -6.02   0.000\n                        Semi-skilled  |\n                                  vs  |\n              Managerial & Technical  |   -3.20972   .3412208    -9.41   0.000\n Unskilled vs Managerial & Technical  |  -3.790512    .591052    -6.41   0.000\n               Skilled vs Non-Manual  |  -.7027994     .33355    -2.11   0.035\n          Semi-skilled vs Non-Manual  |   -1.87564   .3334359    -5.63   0.000\n             Unskilled vs Non-Manual  |  -2.456432   .5864838    -4.19   0.000\n             Semi-skilled vs Skilled  |  -1.172841   .3799212    -3.09   0.002\n                Unskilled vs Skilled  |  -1.753633   .6150853    -2.85   0.004\n           Unskilled vs Semi-skilled  |   -.580792   .6134086    -0.95   0.344\n------------------------------------------------------------------------------\nHere our conclusions don’t change - Unskilled & Semi-skilled have the same marginal means, and Professional and Managerial have the same marginal means, all other comparisons are significant."
  },
  {
    "objectID": "02-fitting-mixed-models.html#assumptions",
    "href": "02-fitting-mixed-models.html#assumptions",
    "title": "2  Fitting Linear Mixed Models",
    "section": "2.2 Assumptions",
    "text": "2.2 Assumptions\nThe linear additivity remains necessary - we need to assume that the true relationship between the predictors and the outcome is linear (as opposed to something more complicated like exponential) and additive (as opposed to multiplicative, unless we are including interactions). With regress, we could use the rvf post-estimation command to generate a plot of residuals versus predicted values. The rvfplot command does not work after mixed, but we can generate it manually.\n. predict fitted, xb\n(524 missing values generated)\n\n. predict residuals, res\n(2,028 missing values generated)\n\n. twoway scatter residuals fitted\n\nThe odd grouping pattern shown is due to the two categorical variables (gender and social class in the model). Each “blob” represents one permutation. You can see this by overlaying many plots:\n. twoway (scatter residuals fitted if female == 1 & socialclass == 1) ///\n&gt;        (scatter residuals fitted if female == 0 & socialclass == 1) ///\n&gt;        (scatter residuals fitted if female == 1 & socialclass == 2) ///\n&gt;        (scatter residuals fitted if female == 0 & socialclass == 2) ///\n&gt;        (scatter residuals fitted if female == 1 & socialclass == 3) ///\n&gt;        (scatter residuals fitted if female == 0 & socialclass == 3) ///\n&gt;        (scatter residuals fitted if female == 1 & socialclass == 4) ///\n&gt;        (scatter residuals fitted if female == 0 & socialclass == 4) ///\n&gt;        (scatter residuals fitted if female == 1 & socialclass == 5) ///\n&gt;        (scatter residuals fitted if female == 0 & socialclass == 5) ///\n&gt;        (scatter residuals fitted if female == 1 & socialclass == 6) ///\n&gt;        (scatter residuals fitted if female == 0 & socialclass == 6), ///\n&gt;        legend(off)\n\nOverall though we see no pattern in the residuals.\nThe other two assumptions which are relevant in linear regression, homogeneity of residuals and independence, are both violated by design in a mixed model. However, you need to assume that no other violations occur - if there is additional variance heterogeneity, such as that brought above by very skewed response variables, you may need to make adjustments. Similarly, if there is some other form of dependence you are not yet modeling, you need to adjust your model to account for it."
  },
  {
    "objectID": "02-fitting-mixed-models.html#predicting-the-random-effects",
    "href": "02-fitting-mixed-models.html#predicting-the-random-effects",
    "title": "2  Fitting Linear Mixed Models",
    "section": "2.3 Predicting the random effects",
    "text": "2.3 Predicting the random effects\nWhile keeping in mind that we do not estimate the random intercepts when fitting this model, we can predict their BLUPS - best linear unbiased predictors. What’s the difference? It’s subtle, but basically we have far less confidence in predicted values than in estimated values. So any confidence intervals will be substantially larger. We know from the model output above that there is variance among the household random intercepts but don’t have a sense of the pattern - is there a single household that’s much different than all the rest? Are they all rather noisy? Or something in between. We can test this.\nFirst, we’ll predict the random intercepts and the standard error of those intercepts.\n. predict rintercept, reffects\n(1,592 missing values generated)\n\n. predict rintse, reses\n(1,592 missing values generated)\nAs noted above, the quality of life variable is missing for around 2000 individuals, so we’ll remove them from the data for ease (calling preserve first to recover the full data later).\n. preserve\n\n. drop if missing(qol)\n(1,670 observations deleted)\nNow within each household, the random intercepts and standard errors are identical, so we can collapse down to the household level.\n. collapse (first) rintercept rintse (count) age, by(household)\nNow we will sort by those random intercepts to make the output look nicer, generate a variable indicating row number for plotting on the x-axis, and compute upper and lower bounds of confidence intervals. Finally we generate a dummy variable to identify which households have confidence intervals not crossing zero.\n. sort rintercept\n\n. gen n = _n\n\n. gen lb = rintercept - 1.96*rintse\n(209 missing values generated)\n\n. gen ub = rintercept + 1.96*rintse\n(209 missing values generated)\n\n. gen significant = (lb &gt; 0 & ub &gt; 0) | (lb &lt; 0 & ub &lt; 0)\nNow we can plot.\n. twoway (rcap ub lb n if significant) ///\n&gt;        (scatter rintercept n if significant), ///\n&gt;          legend(off) yline(0)\n\nThe range in the middle is all households which we predict to have intercepts not distinguishable from zero. We can see that most of the random intercept’s confidence intervals cross with the exception of very few large values and a larger chunk of small values.\nSo there’s a negative skew to the distribution of random intercepts; most households had no additional error, but a decent chunk had significantly lower outcome than we would expect given their personel attributes. We can count exactly how many fall below:\n. count if ub &lt; 0 & lb &lt; 0\n  100"
  },
  {
    "objectID": "02-fitting-mixed-models.html#nested-and-crossed-random-effects",
    "href": "02-fitting-mixed-models.html#nested-and-crossed-random-effects",
    "title": "2  Fitting Linear Mixed Models",
    "section": "2.4 Nested and Crossed random effects",
    "text": "2.4 Nested and Crossed random effects\nThe model we’ve fit so far has a single random intercept, corresponding to household. However, we could have more than one.\n\n2.4.1 Nested random effects\nNested random effects exist when we have a nested level structure. In this data, we actually have this as each household belongs to a single sampling cluster, identified by the cluster variable.\nWe can re-fit the model including a random intercept for cluster, then within each cluster, a random intercept for household. We do so by adding another equation via ||:\n. restore\n\n. mixed qol age agebelow52 ageabove82 i.socialclass female || cluster: || house\n&gt; hold:\n\nPerforming EM optimization ...\n\nPerforming gradient-based optimization: \nIteration 0:  Log likelihood = -17961.965  \nIteration 1:  Log likelihood = -17945.568  \nIteration 2:  Log likelihood =  -17945.22  \nIteration 3:  Log likelihood = -17945.184  \nIteration 4:  Log likelihood = -17945.183  \n\nComputing standard errors ...\n\nMixed-effects ML regression                             Number of obs =  5,179\n\n        Grouping information\n        -------------------------------------------------------------\n                        |     No. of       Observations per group\n         Group variable |     groups    Minimum    Average    Maximum\n        ----------------+--------------------------------------------\n                cluster |        619          1        8.4         28\n              household |      3,995          1        1.3          3\n        -------------------------------------------------------------\n\n                                                        Wald chi2(9)  = 129.84\nLog likelihood = -17945.183                             Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n         qol | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |  -.0102438   .0139236    -0.74   0.462    -.0375336     .017046\n  agebelow52 |  -.3798295   .5275998    -0.72   0.472    -1.413906    .6542472\n  ageabove82 |  -1.210748   .9861846    -1.23   0.220    -3.143634    .7221388\n             |\n socialclass |\nManageria..  |   .2347385   .5116282     0.46   0.646    -.7680344    1.237511\n Non-Manual  |  -1.099335   .5130222    -2.14   0.032     -2.10484   -.0938296\n    Skilled  |  -1.802128   .5375516    -3.35   0.001     -2.85571   -.7485464\nSemi-skil~d  |  -2.974975   .5493449    -5.42   0.000    -4.051671   -1.898278\n  Unskilled  |  -3.555756   .7322564    -4.86   0.000    -4.990952   -2.120559\n             |\n      female |   .5708289   .2086569     2.74   0.006     .1618689    .9797889\n       _cons |   44.78498    1.02202    43.82   0.000     42.78186     46.7881\n------------------------------------------------------------------------------\n\n------------------------------------------------------------------------------\n  Random-effects parameters  |   Estimate   Std. err.     [95% conf. interval]\n-----------------------------+------------------------------------------------\ncluster: Identity            |\n                  var(_cons) |   5.41e-09   1.56e-06      1.2e-254    2.5e+237\n-----------------------------+------------------------------------------------\nhousehold: Identity          |\n                  var(_cons) |    22.9346   1.795878      19.67155    26.73893\n-----------------------------+------------------------------------------------\n               var(Residual) |   38.98675   1.613254      35.94965    42.28043\n------------------------------------------------------------------------------\nLR test vs. linear model: chi2(2) = 146.18                Prob &gt; chi2 = 0.0000\n\nNote: LR test is conservative and provided only for reference.\nOverall the output looks very similar to the model before, but now in the last table, the Random-effects Parameters, we have estimates of three variances, with the addition of cluster random effects.\nIn this case, the estimated variance is almost 0 (so close to 0 that Stata refuses to even compute a confidence interval), so we do not need this random effect - once we control for the fixed effects, there is no additional error common to each cluster. This shouldn’t be too surprising, as the clusters in this data are part of the survey design (which, again, we are ignoring) and somewhat meaningless.\nIf you compare the results of this model to the first model, you’ll see that the results are basically identical - as we expect when adding a predictor that does not improve model fit. It’s up to you whether you want to remove it. On the one hand, you can now appropriately claim you’re controlling for cluster-effects, but on the other hand, an additional random intercept makes the model converge much more slowly.\n\n\n2.4.2 Crossed random effects\nIn the above design, we had nesting - within each cluster there are households, within each household there are individuals. However, this need not always be the case.\nFor example, imagine you lead a large research team where 100 research assistants are conducting repeated surveys of individuals. Here we have repeated measures per person, but we also potentially have interviewer effects that we want to include as random intercepts. However, there is no nesting structure here. Instead, we call this a crossed random effects structure.\nHere’s the rub though: There is no such thing as “nested random effects”. Everything is crossed. So why do nested random effects exist? Consider the following small data set.\n\n\n\nclassroom\nstudentid\n\n\n\n\na\n1\n\n\na\n2\n\n\nb\n1\n\n\nb\n2\n\n\n\nHere the first row of data represents the first student in classroom a, and the third row represents the first student in classroom b - both students are not the same student. If we attempted to tell Stata that these random effects are crossed, Stata will incorrectly think rows 1 and 3 are the same student. By telling Stata that studentid is nested inside classroom, it knows that student 1 from classroom a is distinct from student 1 from classroom b.\nHowever, if we were more clever with our data, we might instead store our data as:\n\n\n\nclassroom\nstudentid\n\n\n\n\na\n1\n\n\na\n2\n\n\nb\n3\n\n\nb\n4\n\n\n\nNow if we tell Stata these are crossed random effects, it won’t get confused! So all nested random effects are just a way to make up for the fact that you may have been foolish in storing your data.\nUnfortunately fitting crossed random effects in Stata is a bit unwieldy. Here’s how we’d fit the model we’ve been working with with crossed random effects.\nmixed qol age agebelow52 ageabove82 i.socialclass female || ///\n          _all:R.household || _all:R.cluster\nHowever, this model will run for a while and then fail - just because functionally crossed and nested effects are the same, does not mean the algorithm which the software uses functions the same. Stata simply fails more often with crossed random effects. On the other hand, the lmer command in R has an easier time of specifying crossed effects and can converge this model just fine."
  },
  {
    "objectID": "02-fitting-mixed-models.html#choosing-random-or-fixed-effects",
    "href": "02-fitting-mixed-models.html#choosing-random-or-fixed-effects",
    "title": "2  Fitting Linear Mixed Models",
    "section": "2.5 Choosing Random or Fixed effects",
    "text": "2.5 Choosing Random or Fixed effects\nWhen these models were first being developed, the recommended guidelines were:\n\nFixed effects are used when the categories in the data represent all possible categories. For example, the collection of all schools in a given state.\nRandom effects are used when the categories in the data represent a sample of all possible categories. For example, a sample of students in a school.\n\nThere are plenty of grey-areas in between so this advice isn’t always useful. Instead think of it from a practical point of view:\n\nDo you have a large enough sample size to include fixed effects (recall the rule of 10-20 observations per predictor. If each person in your data has no more than 2 observations, that’s far too many fixed effects)?\nDo you need to actually estimate the intercept within each group, as opposed to just controlling for group differences? (We can always predict random effects but that’s not as powerful as estimating.)\n\nIf the answer to both these are No, include as random effects.\n(If you don’t need to estimate the group intercepts but the number of groups is low (say less than 5-10), it’s more common to just include it as a fixed effect anyways. But that’s just convention, not due to any strong argument which I’m aware of.)"
  },
  {
    "objectID": "02-fitting-mixed-models.html#miscellaneous",
    "href": "02-fitting-mixed-models.html#miscellaneous",
    "title": "2  Fitting Linear Mixed Models",
    "section": "2.6 Miscellaneous",
    "text": "2.6 Miscellaneous\nThere are various concerns we had with non-mixed models; most still hold. See the previous set of notes (linked to each topic) for more details.\nMulticollinearity, the issue that predictor variables can be correlated amongst each other to provide false positive results, remains an issue in linear regression. The VIF cannot be estimated after a mixed model.\nOverfitting is still possible. Sample size considerations are tricky, but the general rule of 10-20 observations per predictor is a good starting point. There’s the additional complexity of the group structure, but generally there are no restrictions on it. However, in practice, in you have a large number of groups with only a single measurement, convergence issues tend to arise.\nModel selection is still a very bad idea to do.\nRobust standard errors are still obtainable using vce(robust) option."
  },
  {
    "objectID": "02-fitting-mixed-models.html#convergence-issues",
    "href": "02-fitting-mixed-models.html#convergence-issues",
    "title": "2  Fitting Linear Mixed Models",
    "section": "2.7 Convergence issues",
    "text": "2.7 Convergence issues\nWith mixed models, the solution is arrived at iteratively, which means it can fail to converge for a number of reasons. Generally, failure to converge will be due to an issue with the data.\nThe first thing to try is scaling all your parameters:\negen scaled_var = std(var)\nDummy variables typically don’t need to be scaled, but can be. If scaling all your variables allows convergence, try different combinations of scaled and unscaled to figure out what variables are causing the problem. It’s typically (but not always) the variables which have the largest scale to begin with.\nAnother potential convergence issue is extremely high correlation between predictors (including dummy variables). You should have already addressed this when considering multicollinearity, but if not, it can make convergence challenging.\nIf the iteration keeps running (as opposed to ending and complaining about lack of convergence), try passing the option iterate(#) with a few “large” (“large” is relative to running time) numbers to tell the algorithm to stop after # iterations, regardless of convergence. (Recall that an iterative solution produces an answer at each iteration, it’s just not a consistent answer until you reach convergence.) You’re looking for two things:\n\nFirst, if there are any estimated standard errors that are extremely close to zero or exploding towards infinity, that predictor may be causing the issue. Try removing it.\nSecond, if you try a few different max iterations (say 50, 100 and 200), and the estimated coefficients and standard errors are relatively constant, you may be running into a case where the model is converging to just beyond the tolerance which Stata uses, but for all intents and purposes is converged. Set iterate(#) to a high number and use that result.\n\nYou can try use the “reml” optimizer, by passing the reml option. This optimizer can be a bit easier to converge, though may be slower.\nFinally, although it pains me to admit it, you should try running the model in different software such as R or SAS. Each software has slightly different algorithms it uses, and there are situations where one software will converge and another won’t"
  },
  {
    "objectID": "03-random-slopes.html#fitting-a-random-slope",
    "href": "03-random-slopes.html#fitting-a-random-slope",
    "title": "3  Random slopes",
    "section": "3.1 Fitting a random slope",
    "text": "3.1 Fitting a random slope\nLet’s add a random slope for gender.\n. mixed qol age agebelow52 ageabove82 i.socialclass female || household: female\n\nPerforming EM optimization ...\n\nPerforming gradient-based optimization: \nIteration 0:  Log likelihood = -18119.073  (not concave)\nIteration 1:  Log likelihood = -17948.342  \nIteration 2:  Log likelihood =  -17943.72  \nIteration 3:  Log likelihood = -17943.715  \nIteration 4:  Log likelihood = -17943.715  \n\nComputing standard errors ...\n\nMixed-effects ML regression                          Number of obs    =  5,179\nGroup variable: household                            Number of groups =  3,995\n                                                     Obs per group:\n                                                                  min =      1\n                                                                  avg =    1.3\n                                                                  max =      3\n                                                     Wald chi2(9)     = 129.85\nLog likelihood = -17943.715                          Prob &gt; chi2      = 0.0000\n\n------------------------------------------------------------------------------\n         qol | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |  -.0091432   .0139003    -0.66   0.511    -.0363872    .0181008\n  agebelow52 |  -.3693919   .5323218    -0.69   0.488    -1.412723    .6739396\n  ageabove82 |  -1.205023   .9809622    -1.23   0.219    -3.127673    .7176278\n             |\n socialclass |\nManageria..  |   .2311012   .5075454     0.46   0.649    -.7636694    1.225872\n Non-Manual  |  -1.107475   .5090861    -2.18   0.030    -2.105265   -.1096846\n    Skilled  |  -1.807631   .5325279    -3.39   0.001    -2.851366    -.763895\nSemi-skil~d  |  -2.980058   .5455297    -5.46   0.000    -4.049276   -1.910839\n  Unskilled  |  -3.576474   .7302271    -4.90   0.000    -5.007693   -2.145255\n             |\n      female |   .5655497   .2081084     2.72   0.007     .1576647    .9734348\n       _cons |   44.72468   1.018609    43.91   0.000     42.72825    46.72112\n------------------------------------------------------------------------------\n\n------------------------------------------------------------------------------\n  Random-effects parameters  |   Estimate   Std. err.     [95% conf. interval]\n-----------------------------+------------------------------------------------\nhousehold: Independent       |\n                 var(female) |   4.050416   2.356897      1.294774    12.67084\n                  var(_cons) |    22.7137   1.789205      19.46421    26.50567\n-----------------------------+------------------------------------------------\n               var(Residual) |   36.96956   1.960264      33.32041    41.01835\n------------------------------------------------------------------------------\nLR test vs. linear model: chi2(2) = 149.11                Prob &gt; chi2 = 0.0000\n\nNote: LR test is conservative and provided only for reference.\nMost of the output seems very familiar. The only addition is the “var(female)” in the Random-effects Parameters table which, just like in random intercepts, estimates the variance across all the random slopes. Here it is very non-zero, so improves model fit. However, none of the fixed effects really change. The only difference is in the interpretation of the coefficient on female:\nIn the random intercepts model, the coefficient on female represented that females were on average that much higher than males, regardless of age, social class, or inter-household variance.\nIn this model with the random slope as well, the coefficient on female represents the average across all households of the amount that females are above males, regardless of age or social class."
  },
  {
    "objectID": "03-random-slopes.html#do-you-need-to-include-the-fixed-slope-if-you-have-the-random-slopes",
    "href": "03-random-slopes.html#do-you-need-to-include-the-fixed-slope-if-you-have-the-random-slopes",
    "title": "3  Random slopes",
    "section": "3.2 Do you need to include the fixed slope if you have the random slopes",
    "text": "3.2 Do you need to include the fixed slope if you have the random slopes\nYes.\nIn almost every model.\nThis is very similar to excluding the intercept (\\(\\beta_0\\)) in a model - this forces the slope to pass through (0,0). In some very rare situations that might be appropriate, but extremely rarely.\nExcluding the fixed slope when including random slopes forces the average of all random slopes to be 0. If the true random slope is far from zero, this will have catastrophic effects, including reversing the signs on a good number of the random slopes."
  },
  {
    "objectID": "03-random-slopes.html#footnotes",
    "href": "03-random-slopes.html#footnotes",
    "title": "3  Random slopes",
    "section": "",
    "text": "If you’re curious, my rational is I’d rather fit a simpler model that misses a nuanced complexity, then fit a more complicated model that has takes a substantial power hit and potentially is drastically further from the “truth”.↩︎"
  },
  {
    "objectID": "04-generalized-linear-mixed-models.html#logistic-mixed-model",
    "href": "04-generalized-linear-mixed-models.html#logistic-mixed-model",
    "title": "4  Generalized Linear Mixed Models",
    "section": "4.1 Logistic Mixed Model",
    "text": "4.1 Logistic Mixed Model\nThere are actually two commands for logistic mixed models: melogit and meqrlogit. The former is faster, but the latter is more likely to converge. Both commands function generally identically. Note that meqrlogit is a somewhat outdated command, so it’s possible that newer features to melogit may no longer work with meqrlogit.\nSeparation remains a major concern amongst fixed effects, but of lesser concern amongst random intercepts (e.g. a household where everyone had a positive response would break if included as a fixed effect, but generally would run as a random intercept). The only concern is that separation in random effects can make convergence harder to achieve."
  },
  {
    "objectID": "04-generalized-linear-mixed-models.html#poisson-mixed-model",
    "href": "04-generalized-linear-mixed-models.html#poisson-mixed-model",
    "title": "4  Generalized Linear Mixed Models",
    "section": "4.2 Poisson Mixed Model",
    "text": "4.2 Poisson Mixed Model\nPoisson mixed models can be run with the mepoisson command. A meqrpoisson command exists and has benefits just like meqrlogit, but again, is an outdated command. If over-dispersion is an issue, menbreg exists for negative binomial regression."
  },
  {
    "objectID": "04-generalized-linear-mixed-models.html#ordinal-logistic-regression",
    "href": "04-generalized-linear-mixed-models.html#ordinal-logistic-regression",
    "title": "4  Generalized Linear Mixed Models",
    "section": "4.3 Ordinal Logistic Regression",
    "text": "4.3 Ordinal Logistic Regression\nThese models can be run with meologit."
  },
  {
    "objectID": "04-generalized-linear-mixed-models.html#multinomial-logistic-regression",
    "href": "04-generalized-linear-mixed-models.html#multinomial-logistic-regression",
    "title": "4  Generalized Linear Mixed Models",
    "section": "4.4 Multinomial Logistic Regression",
    "text": "4.4 Multinomial Logistic Regression\nThis can only be fit via sem."
  }
]